{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole process begins with raw ModelNet10 data(.OFF file)\n",
    "It only contains endpoints of the model(like those points at each corner).\n",
    "Inorder to get points that evenly spread across all surfacts of the model, we need PointCloudLibrary(PCL) to sample our model.\n",
    "but PCL only accept .PLY file so conversion is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: convert .OFF file to .PLY file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "\n",
    "categories = ['bathtub','bed','chair','desk','dresser','monitor','night_stand','sofa','table','toilet']\n",
    "path = 'c:\\\\Users\\\\sean_\\\\Downloads\\\\ModelNet10\\\\'\n",
    "\n",
    "def OFFtoPLY(path,categories,DataGroup):\n",
    "    for cat  in categories:\n",
    "        DataArray=[]\n",
    "        #deal with train first\n",
    "        files = os.listdir(path + cat + '\\\\'+DataGroup+'\\\\')\n",
    "        files = [x for x in files if x[-4:] == '.off']\n",
    "        for file_index,file in enumerate(files):\n",
    "            fileName = file.split('.')[0]\n",
    "            with open(path + cat + '\\\\'+DataGroup+'\\\\' + file, 'r') as f:\n",
    "                f.readline()\n",
    "                #get number of points in the model\n",
    "                line = f.readline().replace('\\n','')\n",
    "                point_count = line.split(' ')[0]\n",
    "                face_count = line.split(' ')[1]\n",
    "            \n",
    "                #create ply file,write in header first.\n",
    "                with open(path + cat + '\\\\'+DataGroup+'\\\\' + fileName + \".ply\",'w') as plyFile:\n",
    "                    plyFile.write('ply\\nformat ascii 1.0\\nelement vertex ')\n",
    "                    plyFile.write(point_count)\n",
    "                    plyFile.write('\\nproperty float32 x\\nproperty float32 y\\nproperty float32 z\\nelement face ')\n",
    "                    plyFile.write(face_count)\n",
    "                    plyFile.write('\\nproperty list uint8 int32 vertex_indices\\nend_header\\n')\n",
    "                    for index in range(0,int(point_count)+int(face_count)):\n",
    "                        plyFile.write(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OFFtoPLY(path,categories,'train')\n",
    "OFFtoPLY(path,categories,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setp two: call tool \"pcl_mesh_sampling_release.exe\"(for pcl version higher than 1.9.1) to convert all .PLY data to .PCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def PLYtoPCD(path,categories,DataGroup):\n",
    "    for cat  in categories:\n",
    "        DataArray=[]\n",
    "        #deal with train first\n",
    "        files = os.listdir(path + cat + '\\\\'+DataGroup+'\\\\')\n",
    "        files = [x for x in files if x[-4:] == '.ply']\n",
    "        for file_index,file in enumerate(files):\n",
    "            fileName = file.split('.')[0]\n",
    "            subprocess.call(['C:\\\\Users\\\\sean_\\\\Desktop\\\\PLYconv\\\\pcl_mesh_sampling_release.exe',path + cat + '\\\\'+DataGroup+'\\\\' + file,path + cat + '\\\\'+DataGroup+'\\\\' + fileName + \".pcd\",'-no_vis_result','-n_samples', '2200','-leaf_size', '0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PLYtoPCD(path,categories,'train')\n",
    "PLYtoPCD(path,categories,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step three: Merge converted PCD file to one .h5 file the shape of the data should be [n,2048,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCDtoH5(path,categories,DataGroup):\n",
    "    for cat  in categories:\n",
    "        DataArray=[]    \n",
    "        #deal with train first\n",
    "        files = os.listdir(path + cat + '\\\\'+DataGroup+'\\\\')\n",
    "        files = [x for x in files if x[-4:] == '.pcd']\n",
    "        for file_index,file in enumerate(files):\n",
    "            fileName = file.split('.')[0]\n",
    "            with open(path + cat + '\\\\'+DataGroup+'\\\\' + file, 'r') as f:\n",
    "                for y in range(9):\n",
    "                    f.readline()\n",
    "                #get number of points in the model\n",
    "                line = f.readline().replace('\\n','')\n",
    "                point_count = line.split(' ')[1]\n",
    "                #number of data less or more than 2048\n",
    "                pad_count = 2048 - int(point_count)\n",
    "                data = []\n",
    "                f.readline()\n",
    "                #fill ndarray with datapoints\n",
    "                for index in range(0,int(point_count)):\n",
    "                    line = f.readline().rstrip().split()\n",
    "                    line[0] = float(line[0])\n",
    "                    line[1] = float(line[1])\n",
    "                    line[2] = float(line[2])\n",
    "                    data.append(line)\n",
    "                data = np.array(data)\n",
    "                if pad_count > 0 :\n",
    "                    idx = np.random.randint(point_count, size=pad_count)\n",
    "                    data = np.append(data,data[idx],axis=0)\n",
    "                elif  pad_count < 0 :\n",
    "                    index_pool = np.arange(int(point_count))\n",
    "                    np.random.shuffle(index_pool)\n",
    "                    data = data[index_pool[:2048]]\n",
    "                centroid = np.mean(data, axis=0)\n",
    "                data = data - centroid\n",
    "                m = np.max(np.sqrt(np.sum(data**2, axis=1)))\n",
    "                data = data / m\n",
    "                data = np.array([data])\n",
    "            \n",
    "                label = np.array(categories.index(cat)).reshape(1,1)\n",
    "                if file_index == 0 and categories.index(cat) ==0:\n",
    "                    with h5py.File(path + DataGroup +\".h5\", \"w\") as ff:\n",
    "                        ff.create_dataset(name='data', data=data,maxshape=(None, 2048, 3), chunks=True)\n",
    "                        ff.create_dataset(name='label', data=label,maxshape=(None, 1), chunks=True)\n",
    "                else:\n",
    "                    with h5py.File(path +DataGroup +\".h5\", \"a\") as hf:\n",
    "                        hf['data'].resize((hf['data'].shape[0] + 1), axis=0)\n",
    "                        hf['data'][-1:] = data\n",
    "                        hf['label'].resize((hf['label'].shape[0] + 1), axis=0)\n",
    "                        hf['label'][-1:] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCDtoH5(path,categories,'test')\n",
    "PCDtoH5(path,categories,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is something to check data shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "path = 'C:\\\\Users\\\\sean_\\\\Downloads\\\\ModelNet10\\\\'\n",
    "with h5py.File(path +\"test.h5\", 'r') as hf:\n",
    "    data = hf['data'] # <HDF5 dataset>\n",
    "    for point in data:\n",
    "        model1 = pd.DataFrame(point,columns=['x','y','z'])\n",
    "        print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
